{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Untitled5.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "mount_file_id": "17BX7xWCgspKaCtearXBiZWaEcwEUuGMY",
      "authorship_tag": "ABX9TyOj6H2U5G8PnifKvSX/UnMi",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tonyamf/IR/blob/main/Untitled5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ngzdEw-sSH_B"
      },
      "source": [
        "# Cosine simularity"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nxGd7wMunNhR",
        "outputId": "a51c1c38-ce76-4819-b287-d9387ab5cccd"
      },
      "source": [
        "!pip install numba"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: numba in /usr/local/lib/python3.7/dist-packages (0.51.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from numba) (54.2.0)\n",
            "Requirement already satisfied: llvmlite<0.35,>=0.34.0.dev0 in /usr/local/lib/python3.7/dist-packages (from numba) (0.34.0)\n",
            "Requirement already satisfied: numpy>=1.15 in /usr/local/lib/python3.7/dist-packages (from numba) (1.19.5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dtdOYVGz4Auc"
      },
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "import csv\n",
        "import re\n",
        "import time\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import sqlite3\n",
        "import math\n",
        "import sklearn\n",
        "import numpy as np\n",
        "from scipy import spatial\n",
        "from nltk.tokenize import regexp_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "from nltk.corpus import stopwords \n",
        "from nltk.tokenize import regexp_tokenize\n",
        "from nltk.stem import PorterStemmer"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aHyQdBSXCaqT"
      },
      "source": [
        "connection = sqlite3.connect(r\"/content/drive/MyDrive/Colab Notebooks/IR/paper.db\")\n",
        "c = connection.cursor()"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XabKnexlnMCG"
      },
      "source": [
        "from numba import jit\n",
        "\n",
        "@jit(nopython=True, parallel=True)\n",
        "def cosine_similarity_numba(u:np.ndarray, v:np.ndarray):\n",
        "    assert(u.shape[0] == v.shape[0])\n",
        "    uv = 0\n",
        "    uu = 0\n",
        "    vv = 0\n",
        "    for i in range(u.shape[0]):\n",
        "        uv += u[i]*v[i]\n",
        "        uu += u[i]*u[i]\n",
        "        vv += v[i]*v[i]\n",
        "    cos_theta = 1\n",
        "    if uu!=0 and vv!=0:\n",
        "        cos_theta = uv/np.sqrt(uu*vv)\n",
        "    return cos_theta\n",
        "\n",
        "\n",
        "\n",
        "    "
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "754Q8EcHBOFU"
      },
      "source": [
        "lt = connection.execute(\"SELECT * FROM inverted_index\").fetchall()\n",
        "p = connection.execute(\"SELECT * FROM paper\").fetchall()\n",
        "dicpaper = {}\n",
        "dic_ = {}\n",
        "for i in range(0, len(lt)):\n",
        "    lts = regexp_tokenize(lt[i][2], \"[\\w']+\")\n",
        "    results = list(map(int, lts))\n",
        "    ji = 1\n",
        "    if lt[i][1] not in dic_:\n",
        "        dic_[lt[i][1]] = []\n",
        "\n",
        "    for ij in range(0, len(results), 2):\n",
        "        for j in range(ji, results[ij]):\n",
        "            dic_[lt[i][1]].append(0)\n",
        "        tfid = results[ij+1] * math.log10((len(p)*2)/len(results))\n",
        "        dic_[lt[i][1]].append(tfid)\n",
        "        ji = results[ij]+1\n",
        "        # if results[ij] not in dicpaper:\n",
        "        #     dicpaper[results[ij]] = []\n",
        "        # for paps in range(len(dicpaper[results[ij]]), i):\n",
        "        #     dicpaper[results[ij]].append(0)\n",
        "        # dicpaper[results[ij]].append(tfid)  \n",
        "    for j in range(results[ij]+1, len(p)+1):\n",
        "        dic_[lt[i][1]].append(0)\n",
        "    #     for paps in range(0, i):\n",
        "    #         if results[ij] not in dicpaper:\n",
        "    #             dicpaper[results[ij]] = []\n",
        "    #         dicpaper[results[ij]].append(0)\n",
        "    #     if results[ij] not in dicpaper:\n",
        "    #         dicpaper[results[ij]] = []\n",
        "    #     dicpaper[results[ij]].append(tfid)\n",
        "    #     dic_[lt[i][1]].append(tfid)\n",
        "\n",
        "    #    # if j not in dicpaper:\n",
        "    #     #     dicpaper[j] = []\n",
        "    #     # dicpaper[j].append(0)\n",
        "    #     dic_[lt[i][1]].append(0)\n",
        "\n",
        "\n",
        "# for pi in range(1, len(dicpaper)+1):\n",
        "#     for pis in range(len(dicpaper[pi]), len(lt)):\n",
        "#         dicpaper[pi].append(0)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IPX16fCc4PLt"
      },
      "source": [
        "dicpaper = {}\n",
        "for i in range(1,len(p)+1):\n",
        "  for j in range(0, len(lt)):\n",
        "    if i not in dicpaper:\n",
        "      dicpaper[i] = []\n",
        "    dicpaper[i].append(dic_[lt[j][1]][i-1])"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ln7UkA5d6IZD",
        "outputId": "f4a9b9ea-a1da-4966-ae0c-5bfdcc47e0de"
      },
      "source": [
        "len(dicpaper[1])"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "19084"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sHxHeBQ36ZpT",
        "outputId": "ed693129-55c2-476c-d6a6-a53cb2ace012"
      },
      "source": [
        "len(lt)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "19084"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "48mP2XN9Donc",
        "outputId": "11fdbac1-9903-46fc-e77e-22916fac310e"
      },
      "source": [
        "ind = {}\n",
        "ordelis = {}\n",
        "for i in range(1, len(dicpaper)+1):\n",
        "    for j in range(1, len(dicpaper)+1):\n",
        "\n",
        "        # if j == i:\n",
        "        #     continue    \n",
        "        cs = cosine_similarity_numba(np.array(dicpaper[i]), np.array(dicpaper[j]))\n",
        "        #cs=calc_cosine(,)\n",
        "        if i not in ordelis:\n",
        "            ordelis[i] = []\n",
        "        if i not in ind:\n",
        "            ind[i] = []\n",
        "        # if j not in ordelis:\n",
        "        #     ordelis[j] = []\n",
        "        # if j not in ind:\n",
        "        #     ind[j] = [] \n",
        "\n",
        "        ordelis[i].append(cs)\n",
        "        # ordelis[j].append(cs)\n",
        "        ind[i].append(j)\n",
        "        # ind[j].append(i)\n",
        "        for ij in range( len(ordelis[i])-1, 0, -1):\n",
        "            if ordelis[i][ij] > ordelis[i][ij-1]:\n",
        "                temp = ordelis[i][ij]\n",
        "                ordelis[i][ij] = ordelis[i][ij-1]\n",
        "                ordelis[i][ij-1] = temp\n",
        "                temp = ind[i][ij]\n",
        "                ind[i][ij] = ind[i][ij-1]\n",
        "                ind[i][ij-1] = temp\n",
        "            else:\n",
        "                break\n",
        "\n",
        "        # for ij in range(len(ordelis[j])-1, 1, -1):\n",
        "        #     if ordelis[j][ij] > ordelis[j][ij-1]:\n",
        "        #         temp = ordelis[j][ij]\n",
        "        #         ordelis[j][ij] = ordelis[j][ij-1]\n",
        "        #         ordelis[j][ij-1] = temp\n",
        "        #         temp = ind[j][ij]\n",
        "        #         ind[j][ij] = ind[j][ij-1]\n",
        "        #         ind[j][ij-1] = temp\n",
        "        #     else:\n",
        "        #         break"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/numba/core/typed_passes.py:314: NumbaPerformanceWarning: \n",
            "The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible.\n",
            "\n",
            "To find out why, try turning on parallel diagnostics, see https://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help.\n",
            "\n",
            "File \"<ipython-input-38-0e7e742c2320>\", line 4:\n",
            "@jit(nopython=True, parallel=True)\n",
            "def cosine_similarity_numba(u:np.ndarray, v:np.ndarray):\n",
            "^\n",
            "\n",
            "  state.func_ir.loc))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1M75HEoSqS_N"
      },
      "source": [
        "c.execute(\"DROP TABLE cos_similarity\")\n",
        "connection.commit()"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PRgPd8nucxPJ"
      },
      "source": [
        "c.execute('''\n",
        "    CREATE TABLE IF NOT EXISTS \"cos_similarity\"(\n",
        "        wordId INTEGER PRIMARY KEY AUTOINCREMENT NOT NULL,\n",
        "        order_docs TEXT\n",
        "    );     \n",
        "''')\n",
        "connection.commit()"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_WsSPN0YelRu"
      },
      "source": [
        "keys_values = ind.items()\n",
        "\n",
        "new_d = [str(value) for key, value in keys_values]\n"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uFUEFTX8ftTX"
      },
      "source": [
        "for i in range(0, len(ind)):\n",
        "  c.execute('''\n",
        "      INSERT INTO cos_similarity('order_docs') VALUES(?)\n",
        "  ''',(new_d[i],)\n",
        "  )\n",
        "  connection.commit()"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "COI-ry2H3HiW",
        "outputId": "dcc62b03-2d5d-4258-e0db-e63f414f15da"
      },
      "source": [
        "\n",
        "len(ind[1])"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2143"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231
        },
        "id": "qggbKAQBqsFR",
        "outputId": "57d7dece-4a1d-4ebc-fd97-37e7a43d67ed"
      },
      "source": [
        "# check sorted list \n",
        "flag = 0\n",
        "i = 1\n",
        "j =1\n",
        "while j < len(ordelis)+1:\n",
        "  while i < len(ordelis[j]):\n",
        "      if(ordelis[j][i] > ordelis[j][i - 1]):\n",
        "          flag = 1\n",
        "          print (\"No, List is not sorted.\")\n",
        "          break\n",
        "      i += 1\n",
        "      j += 1\n",
        "      \n",
        "# printing result\n",
        "if (not flag) :\n",
        "    print (\"Yes, List is sorted.\")\n",
        "else :\n",
        "    print (\"No, List is not sorted.\")"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-984e499af2a8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mj\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mwhile\u001b[0m \u001b[0mj\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mordelis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m   \u001b[0;32mwhile\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mordelis\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m       \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mordelis\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mordelis\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HqDEeIcwbq4O"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gEXY7iIXAu2z"
      },
      "source": [
        "# There is a frog"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WCU9n0us4M7y",
        "outputId": "03b60d0e-ce73-4244-8ce8-85509db5b92a"
      },
      "source": [
        "source = requests.get('https://scholar.google.co.uk/citations?view_op=view_org&hl=en&org=9117984065169182779').text\n",
        "domain = 'https://scholar.google.co.uk'\n",
        "\n",
        "# linkp = str(link) + '&cstart=0&pagesize=100'\n",
        "soup = BeautifulSoup(source, 'html.parser')\n",
        "next = soup.find('button', class_=\"gs_btnPR gs_in_ib gs_btn_half gs_btn_lsb gs_btn_srt gsc_pgn_pnx\")\n",
        "proflinks =[]\n",
        "\n",
        "while True:\n",
        "  soup = BeautifulSoup(source, 'html.parser')\n",
        "  next = soup.find('button', class_=\"gs_btnPR gs_in_ib gs_btn_half gs_btn_lsb gs_btn_srt gsc_pgn_pnx\")\n",
        "  urlWindow = next.get_attribute_list('onclick')\n",
        "  profs = soup.find_all('h3', class_=\"gs_ai_name\")\n",
        "  for prof in profs:\n",
        "    f = domain + str(prof.a.get_attribute_list('href')[0])\n",
        "    proflinks.append(f)\n",
        "  print(len(proflinks))\n",
        "  if urlWindow[0] is not None:\n",
        "    sula = 'https://scholar.google.co.uk/citations?view_op=view_org&hl=en&org=9117984065169182779'\n",
        "    s = '&after_author='\n",
        "    l = '&astart='\n",
        "    n = urlWindow[0].split(\"\\\\\")\n",
        "    u = n[-3].replace('x3d', '')\n",
        "    a = n[-1].replace('x3d', '')\n",
        "    path  = sula + s + u + l + a\n",
        "    source = requests.get(path).text\n",
        "  else:\n",
        "    break \n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10\n",
            "20\n",
            "30\n",
            "40\n",
            "50\n",
            "60\n",
            "70\n",
            "80\n",
            "90\n",
            "100\n",
            "110\n",
            "120\n",
            "130\n",
            "140\n",
            "150\n",
            "160\n",
            "170\n",
            "180\n",
            "190\n",
            "200\n",
            "210\n",
            "220\n",
            "230\n",
            "240\n",
            "250\n",
            "260\n",
            "270\n",
            "280\n",
            "290\n",
            "300\n",
            "310\n",
            "320\n",
            "330\n",
            "340\n",
            "350\n",
            "360\n",
            "370\n",
            "380\n",
            "390\n",
            "400\n",
            "410\n",
            "420\n",
            "430\n",
            "440\n",
            "450\n",
            "460\n",
            "470\n",
            "480\n",
            "490\n",
            "500\n",
            "510\n",
            "520\n",
            "530\n",
            "540\n",
            "550\n",
            "560\n",
            "570\n",
            "580\n",
            "590\n",
            "600\n",
            "610\n",
            "620\n",
            "630\n",
            "640\n",
            "650\n",
            "660\n",
            "670\n",
            "680\n",
            "690\n",
            "698\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W-LsHKWuI_37"
      },
      "source": [
        "from csv import writer\n",
        "def append_list_as_row(file_name, list_of_elem):\n",
        "    # Open file in append mode\n",
        "    with open(file_name, 'a+', newline='', encoding='utf-8') as write_obj:\n",
        "        # Create a writer object from csv module\n",
        "        csv_writer = writer(write_obj)\n",
        "        # Add contents of list as last row in the csv file\n",
        "        csv_writer.writerow(list_of_elem)\n",
        "d = {'title': \"\", 'authors': \"\", 'date': \"\",'journal': \"\", 'pages':\"\", 'conference': \"\",'publisher': \"\", 'description': \"\", 'num_citations': \"\", 'title_link': \"\"}\n",
        "dfs = pd.DataFrame( columns=d, index=None)\n",
        "#dfs = dfs.append(df, ignore_index=True)\n",
        "dfs.to_csv(r'/content/drive/MyDrive/Colab Notebooks/cralSoup.csv', index_label=False, index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7GSau2W-d_XK"
      },
      "source": [
        "dic={}\n",
        "papersLink = []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qkraLAYPOJDD"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "for i in range(0, len(proflinks)):\n",
        "    source = requests.get(proflinks[i]).text\n",
        "    meta =0\n",
        "    while True: \n",
        "        soup = BeautifulSoup(source, 'html.parser')\n",
        "        next = soup.find_all('td', class_=\"gsc_a_e\")\n",
        "        papers = soup.find_all('td', class_=\"gsc_a_t\")\n",
        "        if len(next) != 1:\n",
        "            for paper in papers:\n",
        "                d = str(paper.a.get_attribute_list('data-href')[0])\n",
        "                f = domain + d\n",
        "                papersLink.append(f)\n",
        "            meta = meta+100\n",
        "            linkp = proflinks[i] + '&cstart='+ str(meta) +'&pagesize=100'\n",
        "            time.sleep(0.6)  \n",
        "            source = requests.get(linkp).text\n",
        "            \n",
        "        else: \n",
        "            break\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c2JZnI19S90r"
      },
      "source": [
        "for i in range(0, len(papersLink)):\n",
        "  # s = str(paper.a.get_text()).strip() + \" \" + str(paper.div.get_text()).strip() \n",
        "  # if s in dic:\n",
        "  #     continue\n",
        "  # dic[s] = 1\n",
        "  time.sleep(0.6)\n",
        "  source = requests.get(papersLink[i]).text\n",
        "  soup = BeautifulSoup(source, 'html.parser')\n",
        "  title = soup.find('div', id=\"gsc_vcd_title\")\n",
        "  fields = soup.find_all('div', class_=\"gsc_vcd_field\")\n",
        "  values = soup.find_all('div', class_=\"gsc_vcd_value\")\n",
        "  df = {'title': [], 'authors': [], 'date': [],'journal': [], 'pages':[], 'conference': [],'publisher': [], 'description': [], 'num_citations': [], 'title_link': []}\n",
        "  df['title'] = title.get_text()\n",
        "  for ii in range(0, len(fields)):\n",
        "      if str(fields[ii].get_text()) == 'Authors':\n",
        "          df['authors'] = str(values[ii].get_text())\n",
        "      elif str(fields[ii].get_text()) == 'Publication date':\n",
        "          df['date']=str(values[ii].get_text())\n",
        "      elif str(fields[ii].get_text()) == 'Journal':\n",
        "          df['journal']=str(values[ii].get_text())\n",
        "      elif str(fields[ii].get_text()) == 'Pages':\n",
        "          df['pages']=str(values[ii].get_text())\n",
        "      elif str(fields[ii].get_text()) == 'Conference':\n",
        "          df['conference']=str(values[ii].get_text())\n",
        "      elif str(fields[ii].get_text()) == 'Publisher':\n",
        "          df['publisher']=str(values[ii].get_text())\n",
        "      elif str(fields[ii].get_text()) == 'Description':\n",
        "          df['description']=str(values[ii].get_text())\n",
        "      elif str(fields[ii].get_text()) == 'Total citations':\n",
        "          df['num_citations']=str(values[ii].get_text())\n",
        "  df['title_link']=f\n",
        "  append_list_as_row('/content/drive/MyDrive/Colab Notebooks/cralSoup.csv', df.values())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        },
        "id": "e5C6gu3icP0R",
        "outputId": "459a7443-1140-4676-8106-37d389e63d36"
      },
      "source": [
        "import json\n",
        "\n",
        "with open('/content/drive/MyDrive/Colab Notebooks/data.json', 'w') as fp:\n",
        "    json.dump(dic, fp)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-7a3ad65efc64>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/MyDrive/Colab Notebooks/data.json'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'dic' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cAx9A1SdRLoF"
      },
      "source": [
        "with open('data.json', 'r') as fp:\n",
        "    data = json.load(fp)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Xy0yv-BYRsZ"
      },
      "source": [
        "d = {'title': \"\", 'authors': \"\", 'date': \"\",'journal': \"\", 'pages':\"\", 'conference': \"\",'publisher': \"\", 'description': \"\", 'num_citations': \"\", 'pdf_link': \"\", 'title_link': \"\"}\n",
        "dfs = pd.DataFrame( columns=d, index=None)\n",
        "#dfs = dfs.append(df, ignore_index=True)\n",
        "dfs.to_csv(r'/content/drive/MyDrive/Colab Notebooks/crawlSoup.csv', index_label=False, index=False)\n",
        "      from csv import writer\n",
        "def append_list_as_row(file_name, list_of_elem):\n",
        "    # Open file in append mode\n",
        "    with open(file_name, 'a+', newline='', encoding='utf-8') as write_obj:\n",
        "        # Create a writer object from csv module\n",
        "        csv_writer = writer(write_obj)\n",
        "        # Add contents of list as last row in the csv file\n",
        "        csv_writer.writerow(list_of_elem)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rtGnZ6UU4S2W",
        "outputId": "841a8b72-8056-4801-8749-1518ffce06fc"
      },
      "source": [
        "%cd \"/content/drive/My Drive/Colab Notebooks/\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/Colab Notebooks\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kicBe6hV4q2K",
        "outputId": "0b95d89b-5943-47e1-e98d-8848366ee1e0"
      },
      "source": [
        "import scrapy\n",
        "import json\n",
        "import re\n",
        "from scrapy_selenium import SeleniumRequest\n",
        "from scrapy.selector import Selector\n",
        "from scrapy.linkextractors import LinkExtractor\n",
        "from scrapy.spiders import CrawlSpider, Rule\n",
        "import pandas as pd\n",
        "from csv import writer\n",
        "\n",
        "\n",
        "class ArticlesSpider(scrapy.Spider):\n",
        "    name = 'articles'\n",
        "    allowed_domains = ['scholar.google.co.uk']\n",
        "    start_urls = ['https://scholar.google.co.uk/citations?view_op=view_org&hl=en&org=9117984065169182779/']\n",
        "   \n",
        "    def parse(self, response):\n",
        "        sula = 'https://scholar.google.co.uk/citations?view_op=view_org&hl=en&org=9117984065169182779'\n",
        "        m = response.xpath('//button[@class=\"gs_btnPR gs_in_ib gs_btn_half gs_btn_lsb gs_btn_srt gsc_pgn_pnx\"]//onclick')\n",
        "        # m = m[0].xpath(\".//@onclick\").get()//*[@id=\"gsc_sa_ccl\"]/div[2]/div/div/h3/a\n",
        "        profs = response.xpath('//*[@id=\"gsc_sa_ccl\"]/div/div/div/h3/a')\n",
        "        for prof in profs:\n",
        "            link = prof.xpath(\".//@href\").get()\n",
        "            meta = 0\n",
        "            linkp = str(link) + '&cstart=0&pagesize=100'\n",
        "            yield response.follow(url=linkp, callback= self.parse_prof,meta={'meta': meta})\n",
        "        if m:\n",
        "            s = '&after_author'\n",
        "            l = '&astart='\n",
        "            u = re.sub(r'_author\\x3d.+\\x26','', str(m))\n",
        "            a = re.sub(r'window.+\\x3d','',str(m))\n",
        "            path  = sula + s + u + l + a\n",
        "            scrapy.Request(path, callback=self.parse)\n",
        "\n",
        "    \n",
        "    \n",
        "    def parse_prof(self, response):\n",
        "        meta = response.request.meta['meta']\n",
        "        if len(response.xpath('//*[@id=\"gsc_a_b\"]')) != 0:\n",
        "            for i in range(meta+1, len(response.xpath('//*[@id=\"gsc_a_b\"]'))+1):  \n",
        "                paper = response.xpath('//*[@id=\"gsc_a_b\"]/tr['+str(i)+']/td[1]/a')\n",
        "                pro = paper.get_attribute('data-href')\n",
        "                yield response.follow(url=pro, callback= self.parse_prof,meta={'meta': meta})\n",
        "            meta = meta + 100\n",
        "            linku = response.request.url\n",
        "            lin = re.sub(r'&cstart=.+&pagesize=100','',linku)\n",
        "            linkp = str(lin) + '&cstart='+ str(meta) +'&pagesize=100'\n",
        "            yield scrapy.Request(url=linkp, callback= self.parse_prof,meta={'meta': meta})\n",
        "        \n",
        "   \n",
        "\n",
        "    def parse_paper(self, response):\n",
        "        df = {'title': [], 'authors': [], 'date': [],'journal': [], 'pages':[], 'conference': [],'publisher': [], 'description': [], 'num_citations': [], 'pdf_link': [], 'title_link': []}\n",
        "        body = response.xpath('//*[@id=\"gsc_vcd_table\"]')\n",
        "        for ii in range(1, len(body)+1):\n",
        "            if response.xpath('//*[@id=\"gsc_vcd_table\"]/div['+str(ii)+']/div[1]').text == 'Authors':\n",
        "                df['authors'].append(response.xpath('//*[@id=\"gsc_vcd_table\"]/div['+str(ii)+']/div[2]').text)\n",
        "            elif response.xpath('//*[@id=\"gsc_vcd_table\"]/div['+str(ii)+']/div[1]').text == 'Publication date':\n",
        "                df['date'].append(response.xpath('//*[@id=\"gsc_vcd_table\"]/div['+str(ii)+']/div[2]').text)\n",
        "\n",
        "            elif response.xpath('//*[@id=\"gsc_vcd_table\"]/div['+str(ii)+']/div[1]').text == 'Journal':\n",
        "                df['journal'].append(response.xpath('//*[@id=\"gsc_vcd_table\"]/div['+str(ii)+']/div[2]').text)\n",
        "\n",
        "            elif response.xpath('//*[@id=\"gsc_vcd_table\"]/div['+str(ii)+']/div[1]').text == 'Publisher':\n",
        "                df['publisher'].append(response.xpath('//*[@id=\"gsc_vcd_table\"]/div['+str(ii)+']/div[2]').text)\n",
        "\n",
        "            elif response.xpath('//*[@id=\"gsc_vcd_table\"]/div['+str(ii)+']/div[1]').text == 'Description':\n",
        "                df['description'].append(response.xpath('//*[@id=\"gsc_vcd_table\"]/div['+str(ii)+']/div[2]').text)\n",
        "\n",
        "            elif response.xpath('//*[@id=\"gsc_vcd_table\"]/div['+str(ii)+']/div[1]').text == 'Total citations':\n",
        "                df['num_citations'].append(response.xpath('//*[@id=\"gsc_vcd_table\"]/div['+str(ii)+']/div[2]/div[1]/a').text)\n",
        "\n",
        "            elif response.xpath('//*[@id=\"gsc_vcd_table\"]/div['+str(ii)+']/div[1]').text == 'Pages':\n",
        "                df['pages'].append(response.xpath('//*[@id=\"gsc_vcd_table\"]/div['+str(ii)+']/div[2]').text)\n",
        "\n",
        "            elif response.xpath('//*[@id=\"gsc_vcd_table\"]/div['+str(ii)+']/div[1]').text == 'Conference':\n",
        "                df['conference'].append(response.xpath('//*[@id=\"gsc_vcd_table\"]/div['+str(ii)+']/div[2]').text) \n",
        "                                    \n",
        "        if len(df['pdf_link']) != len( df['authors']):\n",
        "            df['authors'].append('N/A')   \n",
        "        if len(df['pdf_link']) != len( df['date']):\n",
        "            df['date'].append('N/A')   \n",
        "        if len(df['pdf_link']) != len( df['journal']):\n",
        "            df['journal'].append('N/A')   \n",
        "        if len(df['pdf_link']) != len( df['publisher']):\n",
        "            df['publisher'].append('N/A')  \n",
        "        if len(df['pdf_link']) != len( df['description']):\n",
        "            df['description'].append('N/A')\n",
        "        if len(df['pdf_link']) != len( df['num_citations']):\n",
        "            df['num_citations'].append('N/A')\n",
        "        if len(df['pdf_link']) != len( df['pages']):\n",
        "            df['pages'].append('N/A')\n",
        "        if len(df['pdf_link']) != len( df['conference']):\n",
        "            df['conference'].append('N/A')\n",
        "        with open('/content/drive/MyDrive/Colab Notebooks/crawl.csv', 'a+', newline='', encoding='utf-8') as write_obj:\n",
        "            # Create a writer object from csv module\n",
        "            csv_writer = writer(write_obj)\n",
        "            # Add contents of list as last row in the csv file\n",
        "            csv_writer.writerow(df.values())\n",
        "      "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Error: scrapy.cfg already exists in /content/drive/My Drive/Colab Notebooks/google_crawler\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fkkhuQSN5YGd",
        "outputId": "192f32ce-2cdf-4bb5-d0be-f794d88734b6"
      },
      "source": [
        "!mv google_crawler \"/content/drive/My Drive/Colab Notebooks/\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mv: cannot stat 'google_crawler': No such file or directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ASMkjHcP5wgK"
      },
      "source": [
        "import scrapy \n",
        "from scrapy.linkextractors import LinkExtractor\n",
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mwa8Vp_e6KrI"
      },
      "source": [
        "class ArticlesSpider(scrapy.Spider):\n",
        "    name = 'articles'\n",
        "    allowed_domains = ['scholar.google.co.uk']\n",
        "    start_urls = ['https://scholar.google.co.uk/citations?view_op=view_org&hl=en&org=9117984065169182779']\n",
        "   \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wd09IxVFjbm8"
      },
      "source": [
        "    def parse(self, response):\n",
        "        sula = 'https://scholar.google.co.uk/citations?view_op=view_org&hl=en&org=9117984065169182779'\n",
        "        m = response.xpath('//*[@id=\"gsc_authors_bottom_pag\"]/div/button[2]//@onclick')\n",
        "        profs = response_obj.xpath('//*[@id=\"gsc_sa_ccl\"]/div/div/div/h3/a')\n",
        "        for prof in profs:\n",
        "            link = prof.xpath(\".//@href\").get()\n",
        "            meta = 0\n",
        "            linkp = str(link) + '&cstart=0&pagesize=100'\n",
        "            yield response.follow(url=linkp, callback= self.parse_prof,meta={'meta': meta})\n",
        "        if m:\n",
        "            s = '&after_author'\n",
        "            l = '&astart='\n",
        "            u = re.sub(r'_author\\x3d.+\\x26','', str(m))\n",
        "            a = re.sub(r'window.+\\x3d','',str(m))\n",
        "            path  = sula + s + u + l + a\n",
        "            scrapy.Request(path, callback=self.parse)\n",
        "\n",
        "    \n",
        "    \n",
        "    def parse_prof(self, response):\n",
        "        meta = response.request.meta['meta']\n",
        "        if len(response.xpath('//*[@id=\"gsc_a_b\"]')) != 0:\n",
        "            for i in range(meta+1, len(response.xpath('//*[@id=\"gsc_a_b\"]'))+1):  \n",
        "                paper = driver.find_elements_by_xpath('//*[@id=\"gsc_a_b\"]/tr['+str(i)+']/td[1]/a')\n",
        "                pro = paper.get_attribute('data-href')\n",
        "                yield response.follow(url=linkp, callback= self.parse_prof,meta={'meta': meta})\n",
        "            meta = meta + 100\n",
        "            linku = response.request.url\n",
        "            lin = re.sub(r'&cstart=.+&pagesize=100','',linku)\n",
        "            linkp = str(lin) + '&cstart='+ str(meta) +'&pagesize=100'\n",
        "            yield scrapy.Request(url=linkp, callback= self.parse_prof,meta={'meta': meta})\n",
        "        \n",
        "   \n",
        "\n",
        "    def parse_paper(self, response):\n",
        "        df = []\n",
        "        body = response.xpath('//*[@id=\"gsc_vcd_table\"]/div/div[1]')\n",
        "        for ii in range(1, len(body)+1):\n",
        "            if response.xpath('//*[@id=\"gsc_vcd_table\"]/div['+str(ii)+']/div[1]').text == 'Authors':\n",
        "                df['authors'].append(response.xpath('//*[@id=\"gsc_vcd_table\"]/div['+str(ii)+']/div[2]').text)\n",
        "            elif response.xpath('//*[@id=\"gsc_vcd_table\"]/div['+str(ii)+']/div[1]').text == 'Publication date':\n",
        "                df['date'].append(response.xpath('//*[@id=\"gsc_vcd_table\"]/div['+str(ii)+']/div[2]').text)\n",
        "\n",
        "            elif response.xpath('//*[@id=\"gsc_vcd_table\"]/div['+str(ii)+']/div[1]').text == 'Journal':\n",
        "                df['journal'].append(response.xpath('//*[@id=\"gsc_vcd_table\"]/div['+str(ii)+']/div[2]').text)\n",
        "\n",
        "            elif response.xpath('//*[@id=\"gsc_vcd_table\"]/div['+str(ii)+']/div[1]').text == 'Publisher':\n",
        "                df['publisher'].append(response.xpath('//*[@id=\"gsc_vcd_table\"]/div['+str(ii)+']/div[2]').text)\n",
        "\n",
        "            elif response.xpath('//*[@id=\"gsc_vcd_table\"]/div['+str(ii)+']/div[1]').text == 'Description':\n",
        "                df['description'].append(response.xpath('//*[@id=\"gsc_vcd_table\"]/div['+str(ii)+']/div[2]').text)\n",
        "\n",
        "            elif response.xpath('//*[@id=\"gsc_vcd_table\"]/div['+str(ii)+']/div[1]').text == 'Total citations':\n",
        "                df['num_citations'].append(response.xpath('//*[@id=\"gsc_vcd_table\"]/div['+str(ii)+']/div[2]/div[1]/a').text)\n",
        "\n",
        "            elif response.xpath('//*[@id=\"gsc_vcd_table\"]/div['+str(ii)+']/div[1]').text == 'Pages':\n",
        "                df['pages'].append(response.xpath('//*[@id=\"gsc_vcd_table\"]/div['+str(ii)+']/div[2]').text)\n",
        "\n",
        "            elif response.xpath('//*[@id=\"gsc_vcd_table\"]/div['+str(ii)+']/div[1]').text == 'Conference':\n",
        "                df['conference'].append(response.xpath('//*[@id=\"gsc_vcd_table\"]/div['+str(ii)+']/div[2]').text) \n",
        "                                    \n",
        "        if len(df['pdf_link']) != len( df['authors']):\n",
        "            df['authors'].append('N/A')   \n",
        "        if len(df['pdf_link']) != len( df['date']):\n",
        "            df['date'].append('N/A')   \n",
        "        if len(df['pdf_link']) != len( df['journal']):\n",
        "            df['journal'].append('N/A')   \n",
        "        if len(df['pdf_link']) != len( df['publisher']):\n",
        "            df['publisher'].append('N/A')  \n",
        "        if len(df['pdf_link']) != len( df['description']):\n",
        "            df['description'].append('N/A')\n",
        "        if len(df['pdf_link']) != len( df['num_citations']):\n",
        "            df['num_citations'].append('N/A')\n",
        "        if len(df['pdf_link']) != len( df['pages']):\n",
        "            df['pages'].append('N/A')\n",
        "        if len(df['pdf_link']) != len( df['conference']):\n",
        "            df['conference'].append('N/A')\n",
        "        with open('/content/drive/MyDrive/Colab Notebooks/crawl.csv', 'a+', newline='', encoding='utf-8') as write_obj:\n",
        "            # Create a writer object from csv module\n",
        "            csv_writer = writer(write_obj)\n",
        "            # Add contents of list as last row in the csv file\n",
        "            csv_writer.writerow(df)\n",
        "      "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "miYq5oKH6eOq"
      },
      "source": [
        "def parse(self, response):\n",
        "    xlink = LinkExtractor()\n",
        "    for link in xlink.extract_links(response):\n",
        "      print(link)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MVEZtPkBh1aK",
        "outputId": "88ebb1f4-0fec-40ac-87ad-7fc40e776046"
      },
      "source": [
        "cd /content/drive/MyDrive/Colab Notebooks/google_crawler/google_crawler/spiders/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/Colab Notebooks/google_crawler/google_crawler/spiders\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DMcPec_z6u-G",
        "outputId": "99718543-d6af-49dd-9b10-443c5ffff6a3"
      },
      "source": [
        "!scrapy crawl articles"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-04-06 00:21:48 [scrapy.utils.log] INFO: Scrapy 2.4.1 started (bot: google_crawler)\n",
            "2021-04-06 00:21:48 [scrapy.utils.log] INFO: Versions: lxml 4.2.6.0, libxml2 2.9.8, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 21.2.0, Python 3.7.10 (default, Feb 20 2021, 21:17:23) - [GCC 7.5.0], pyOpenSSL 20.0.1 (OpenSSL 1.1.1k  25 Mar 2021), cryptography 3.4.7, Platform Linux-4.19.112+-x86_64-with-Ubuntu-18.04-bionic\n",
            "2021-04-06 00:21:48 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.epollreactor.EPollReactor\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scrapy/spiderloader.py\", line 75, in load\n",
            "    return self._spiders[spider_name]\n",
            "KeyError: 'articles'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/scrapy\", line 8, in <module>\n",
            "    sys.exit(execute())\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scrapy/cmdline.py\", line 145, in execute\n",
            "    _run_print_help(parser, _run_command, cmd, args, opts)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scrapy/cmdline.py\", line 100, in _run_print_help\n",
            "    func(*a, **kw)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scrapy/cmdline.py\", line 153, in _run_command\n",
            "    cmd.run(args, opts)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scrapy/commands/crawl.py\", line 22, in run\n",
            "    crawl_defer = self.crawler_process.crawl(spname, **opts.spargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scrapy/crawler.py\", line 191, in crawl\n",
            "    crawler = self.create_crawler(crawler_or_spidercls)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scrapy/crawler.py\", line 224, in create_crawler\n",
            "    return self._create_crawler(crawler_or_spidercls)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scrapy/crawler.py\", line 228, in _create_crawler\n",
            "    spidercls = self.spider_loader.load(spidercls)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scrapy/spiderloader.py\", line 77, in load\n",
            "    raise KeyError(f\"Spider not found: {spider_name}\")\n",
            "KeyError: 'Spider not found: articles'\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}